{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coursera_3103.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUxuyo42mj96Lz+S7VvPTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawatpremsingh999/tensorflow-coursera/blob/master/coursera_3103.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKPKrYbzsoA"
      },
      "source": [
        "### Sentiment in Texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJIPz1HvvXo"
      },
      "source": [
        "import tensorflow\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilvnuyIoyvpY"
      },
      "source": [
        "**Tokenization of sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-78EcwDDwalO"
      },
      "source": [
        "sentences = ['I Love my parents','I Love my country']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjqMRL9KwybW"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgB9SC8Yw6ot"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpVezJcaxChY"
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6E1kQnoxK3c",
        "outputId": "a27fd8d7-4ddc-420e-c30a-88a4781ebf6d"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'parents': 4, 'country': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96yG6lAryjez"
      },
      "source": [
        "**Take a look at new set of sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5cx8VspxMf-",
        "outputId": "9e07b066-b1b4-4d90-a877-de6c218befc2"
      },
      "source": [
        "sentences2 = ['I Love my parents','I love my country','i, Love your dog!']\r\n",
        "tokenizer.fit_on_texts(sentences2)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'parents': 4, 'country': 5, 'your': 6, 'dog': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX3rPPfA051x"
      },
      "source": [
        "**Get Numerical form of sentences: Encoded into Numbers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjrKrjAx-e2"
      },
      "source": [
        "sentences3 = ['I Love my parents',\r\n",
        "              'I love my country',\r\n",
        "              'i, Love your dog!',\r\n",
        "              'Do you also like my dog?',\r\n",
        "              'Do you think that my dog is amazing?']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vVWPcdk1beW"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)\r\n",
        "tokenizer.fit_on_texts(sentences3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVR8WIV1kTx"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F__oE57h1uI4",
        "outputId": "f59d783b-e2ef-496a-bd6a-920aa75f2782"
      },
      "source": [
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'i': 2, 'love': 3, 'dog': 4, 'do': 5, 'you': 6, 'parents': 7, 'country': 8, 'your': 9, 'also': 10, 'like': 11, 'think': 12, 'that': 13, 'is': 14, 'amazing': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "825m-bST152T",
        "outputId": "08181800-dcff-4137-c79a-3c9b109fae5d"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences3)\r\n",
        "print(sequences) \r\n",
        "# encoded sentences into numerical sequences"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 1, 7], [2, 3, 1, 8], [2, 3, 9, 4], [5, 6, 10, 11, 1, 4], [5, 6, 12, 13, 1, 4, 14, 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygw-e6Nj3ROe"
      },
      "source": [
        "Let's take a test sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GAV7goE2Mfd",
        "outputId": "b2a12f27-1b6f-4b9f-bdea-05bbd875ace8"
      },
      "source": [
        "test_sentences = ['Hello! I am prem',\r\n",
        "                  'I Love to my country',\r\n",
        "                  'I do not have any dog!',\r\n",
        "                  'but I love to keep dog in my house']\r\n",
        "\r\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\r\n",
        "print(test_seq)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2], [2, 3, 1, 8], [2, 5, 4], [2, 3, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_cg9yHU4p_C"
      },
      "source": [
        "**Looking More at tokenizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leNDvZyT6nb2"
      },
      "source": [
        "Add < OOV > token where unseen words are present"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ean81fJO4OpX",
        "outputId": "7b707b81-faf3-4f32-e285-c18a3f0187e9"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100,oov_token=\"<OOV>\")\r\n",
        "\r\n",
        "sentences4 = ['I love my Dog',\r\n",
        "              'I Love my country!',\r\n",
        "              'Dog is a loyal animal',\r\n",
        "              'large number of dog in our country']\r\n",
        "\r\n",
        "tokenizer.fit_on_texts(sentences4)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'dog': 2, 'i': 3, 'love': 4, 'my': 5, 'country': 6, 'is': 7, 'a': 8, 'loyal': 9, 'animal': 10, 'large': 11, 'number': 12, 'of': 13, 'in': 14, 'our': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fVf3het7ttb",
        "outputId": "480079b7-f2e7-498a-f3f5-e9d2996e9850"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences4)\r\n",
        "print(sequences)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 4, 5, 2], [3, 4, 5, 6], [2, 7, 8, 9, 10], [11, 12, 13, 2, 14, 15, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuGPu_T-JL7l",
        "outputId": "ee722fc8-3d05-481c-8116-4f653b89e11d"
      },
      "source": [
        "test_sentences = ['Hello! I am prem',\r\n",
        "                  'I Love to my country',\r\n",
        "                  'I do not have any dog!',\r\n",
        "                  'but I love to keep dog in my house']\r\n",
        "\r\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\r\n",
        "print(test_seq)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 3, 1, 1], [3, 4, 1, 5, 6], [3, 1, 1, 1, 1, 2], [1, 3, 4, 1, 1, 2, 14, 5, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJvXQPuqKWCo"
      },
      "source": [
        "**Pad the sequences**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6wP4K_EKncs"
      },
      "source": [
        "Once the tokenizer create the sequences then we pass to the pad_sequences!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtZpGzDXKau3"
      },
      "source": [
        "# import padding function\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhUeSNjXLcMg",
        "outputId": "ce6835ad-7de1-4e34-97c2-bffc48eee979"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3, 4, 5, 2], [3, 4, 5, 6], [2, 7, 8, 9, 10], [11, 12, 13, 2, 14, 15, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lVKh57ZLIW-",
        "outputId": "6a1de54e-f2a8-4a00-b10c-d3f786dea6f1"
      },
      "source": [
        "padded_seq = pad_sequences(sequences)\r\n",
        "print(padded_seq)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  3  4  5  2]\n",
            " [ 0  0  0  3  4  5  6]\n",
            " [ 0  0  2  7  8  9 10]\n",
            " [11 12 13  2 14 15  6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qah77YyL3hT"
      },
      "source": [
        "If we want to pad the sequences after the actual sequence then we can use 'post' parameter in the pad_sequences function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd05AQFOLYvB",
        "outputId": "3c93288c-5bff-4c22-88b7-10df1cb9bedc"
      },
      "source": [
        "padded_seq_after = pad_sequences(sequences=sequences,padding='post')\r\n",
        "print(padded_seq_after)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  4  5  2  0  0  0]\n",
            " [ 3  4  5  6  0  0  0]\n",
            " [ 2  7  8  9 10  0  0]\n",
            " [11 12 13  2 14 15  6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOdAWwSMaN0"
      },
      "source": [
        "We can also use max length of padded sequence by setting up mexlen parameter in pad_sequences function and In this we lose the words from beggining values of the sequence if particular sentence length is more than maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdBvRj-XMVN8",
        "outputId": "202b0c5f-fe0b-44e6-e45a-1d40269e690a"
      },
      "source": [
        "padded_seq_after_maxlen = pad_sequences(sequences,maxlen=5,padding='post')\r\n",
        "print(padded_seq_after_maxlen)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  4  5  2  0]\n",
            " [ 3  4  5  6  0]\n",
            " [ 2  7  8  9 10]\n",
            " [13  2 14 15  6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9McMNLvONIJZ",
        "outputId": "b823b6fb-61a7-4182-b556-56a6f8f31445"
      },
      "source": [
        "padded_seq_before_maxlen = pad_sequences(sequences,maxlen=5,padding='pre')\r\n",
        "print(padded_seq_before_maxlen)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  3  4  5  2]\n",
            " [ 0  3  4  5  6]\n",
            " [ 2  7  8  9 10]\n",
            " [13  2 14 15  6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyML8SGEOVw7"
      },
      "source": [
        "If we set truncating parameter to 'post' then we will loss information from end of the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M5Pp2fnNUen",
        "outputId": "53965a33-c212-4e44-d85b-063ae4b6f94d"
      },
      "source": [
        "padded_seq_endloss = pad_sequences(sequences,maxlen=5,padding='post',truncating='post')\r\n",
        "print(padded_seq_endloss)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3  4  5  2  0]\n",
            " [ 3  4  5  6  0]\n",
            " [ 2  7  8  9 10]\n",
            " [11 12 13  2 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlWT-lyKQQrR"
      },
      "source": [
        "**Padding on Seen Texts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL4_DkxKOs8l",
        "outputId": "d034f377-cb5f-4a12-8580-f4ed62b83c83"
      },
      "source": [
        "corpus = ['My country is india',\r\n",
        "          'It lies in the continent of Asia.',\r\n",
        "          'India is a beautiful country.',\r\n",
        "          'Capital of India is New Delhi.',\r\n",
        "          'The national language of India is Hindi.']\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=100,oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(corpus)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "corpus_seq = tokenizer.texts_to_sequences(corpus)\r\n",
        "\r\n",
        "padded_corpus = pad_sequences(corpus_seq)\r\n",
        "\r\n",
        "print(\"Corpus words index: \\n\",word_index)\r\n",
        "print(\"\\nSequences of corpus: \\n\",corpus_seq)\r\n",
        "print(\"\\nPadded Sequences: \\n\",padded_corpus)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus words index: \n",
            " {'<OOV>': 1, 'is': 2, 'india': 3, 'of': 4, 'country': 5, 'the': 6, 'my': 7, 'it': 8, 'lies': 9, 'in': 10, 'continent': 11, 'asia': 12, 'a': 13, 'beautiful': 14, 'capital': 15, 'new': 16, 'delhi': 17, 'national': 18, 'language': 19, 'hindi': 20}\n",
            "\n",
            "Sequences of corpus: \n",
            " [[7, 5, 2, 3], [8, 9, 10, 6, 11, 4, 12], [3, 2, 13, 14, 5], [15, 4, 3, 2, 16, 17], [6, 18, 19, 4, 3, 2, 20]]\n",
            "\n",
            "Padded Sequences: \n",
            " [[ 0  0  0  7  5  2  3]\n",
            " [ 8  9 10  6 11  4 12]\n",
            " [ 0  0  3  2 13 14  5]\n",
            " [ 0 15  4  3  2 16 17]\n",
            " [ 6 18 19  4  3  2 20]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YEroEEtSzHG"
      },
      "source": [
        "**Padding on Unseen Texts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sRk0cryRwa3",
        "outputId": "2d5ee1d0-0307-45e5-a59d-ad69c7d7ed64"
      },
      "source": [
        "test_corpus = ['I love my country!',\r\n",
        "               'India is a famous country all over the world.',\r\n",
        "               'India is a democratic country.',\r\n",
        "               'Peacocks look beautiful in their colorful feathers.',\r\n",
        "               'Peacock is the most beautiful creatures of the earth']\r\n",
        "\r\n",
        "test_seq = tokenizer.texts_to_sequences(test_corpus)\r\n",
        "print(\"Test Sequences: \\n\",test_seq)\r\n",
        "\r\n",
        "pad_seq = pad_sequences(test_seq,maxlen=5)\r\n",
        "print(\"Padded Test Sequences: \\n\",pad_seq)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Sequences: \n",
            " [[1, 1, 7, 5], [3, 2, 13, 1, 5, 1, 1, 6, 1], [3, 2, 13, 1, 5], [1, 1, 14, 10, 1, 1, 1], [1, 2, 6, 1, 14, 1, 4, 6, 1]]\n",
            "Padded Test Sequences: \n",
            " [[ 0  1  1  7  5]\n",
            " [ 5  1  1  6  1]\n",
            " [ 3  2 13  1  5]\n",
            " [14 10  1  1  1]\n",
            " [14  1  4  6  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8cNLBWaT_P-"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}